# -*- coding: utf-8 -*-
"""Readability classification model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YAT8YX_2ZIjDCKfV7ECvH93N3CQ3e66-
"""

#To install the latest version of Xgboost in colab
# !pip uninstall xgboost
# !pip install textstat
# !pip install xgboost

#Import needed libraries
from textstat.textstat import textstatistics, textstat, legacy_round 
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
from lightgbm import LGBMClassifier
import matplotlib.pyplot as plt
from google.colab import drive
import xgboost as xgb
import pkg_resources
import pandas as pd
import pickle as pkl
import spacy 
import nltk
import os

#check for xgboost version
xgb.__version__

#download nltk for word tokenization
nltk.download('punkt')

#mount google drive to use with colab
drive.mount('/content/gdrive')
path = 'gdrive/My Drive/Readability_data/'
level_1_path = path + 'Int-Txt'
level_2_path = path + 'Ele-Txt'
level_3_path = path + 'Adv-Txt'

#This function saves model as pickle file
def save_pkl(filename,obj):
  pkl.dump(obj, open(filename, 'wb'))

#This function loads model as pickle file 
def load_pkl(filename):
  return(pkl.load(open(filename,'rb')))

#Get all file in given director
def get_files_in_dir(path):

  files = []

  #Loop for every file in given path, gets and appends its name
  for r, d, f in os.walk(path):
    for file in f:
      if '.txt' in file:
        files.append(os.path.join(r, file))

  return files

#Remove any non ascii character in given text
def remove_non_ascii(text):
  return ''.join([i if ord(i) < 128 and i != '\n' else ' ' for i in text])
    
#Return all text in given file
def read_file(file_path):
  try:
    with open(file_path, 'r') as file:
      text = ''
      for line in file.readlines():
        text += remove_non_ascii(line)
      return text
  except:
    print("file was not found!  "+file_path)
    return ''

#Gets all files for elementary level
files_level_1 = get_files_in_dir(level_1_path)

#Gets all files for intermediate level
files_level_2 = get_files_in_dir(level_2_path)

#Gets all files for advanced level
files_level_3 = get_files_in_dir(level_3_path)

#Load all data from each directory and gives tthem label according to its label

text_data = []
text_labels = []

for file_path in files_level_1:
  data = read_file(file_path)
  #Checks for empty data of an empty file
  if data != '':
    text_data.append(data)
    #Label 0 is given for elementary level
    text_labels.append(0)

for file_path in files_level_2:
  data = read_file(file_path)
  #Checks for empty data of an empty file
  if data != '':
    text_data.append(data)
    #Label 1 is given for intermediate level
    text_labels.append(1)

for file_path in files_level_3:
  data = read_file(file_path)
  #Checks for empty data of an empty file
  if data != '':
    text_data.append(data)
    #Label 2 is given for advanced level
    text_labels.append(2)

#Check for the number of data and labels
print(len(text_data))
print(len(text_labels))

#Inspect the first data int the list
print(text_data[0])
print(text_labels[0])

#Extract all text dependent features
def text_features(text):
  
  #Load spacy library for nlp to extract featurs from
  nlp = spacy.load('en') 
  doc = nlp(text)
  sentences = doc.sents

  #create 2 counters and an empty list
  words_no = 0
  sentences_no = 0
  words = []

  #Count number of words and sentences for a given text
  for sentence in sentences: 
    words_no += len([token for token in sentence]) 
    sentences_no += 1

  #Empty text to append all position and tags of a given text
  tag = ''
  pos = ''
  #Loop on all tokens
  for token in doc:
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)
    
    #Get all tags and pos of a text in a string to count spexific characters
    tag += token.tag_ + ' '
    pos += token.pos_ + ' '
  
  # print(tag)
  # print(pos)

  #Count number of commas in given text / total number of sentences 
  commas_no = tag.count(',') / sentences_no
  #Count number of pronouns in given text / total number of sentences 
  pronouns_no = (tag.count('PRP') + tag.count('PRP$')) / sentences_no
  #Count number of verb as modal auxiliary in given text / total number of sentences 
  modal_verbs_no = tag.count('MD') / sentences_no
  #Count number of personal pronouns in given text / total number of sentences 
  personal_pronouns_no = tag.count('PRP') / sentences_no
  #Count number of wh-pronouns in given text / total number of sentences 
  wh_pronouns_no = (tag.count('WP') + tag.count('WP$')) / sentences_no
  #Count number of function words in given text / total number of sentences 
  function_words_no = (tag.count('BES') + tag.count('CC') + tag.count('DT') \
                       + tag.count('EX') + tag.count('HVS') + tag.count('IN') \
                       + tag.count('MD') + tag.count('PRP') + tag.count('PRP$') \
                       + tag.count('RP') + tag.count('TO') + tag.count('UH')) \
                       / sentences_no
  #Count number of verb in base form in given text / total number of sentences 
  VB_tags_no = tag.count('VB') / sentences_no
  #Count number of 	noun as singular or mass in given text / total number of sentences
  NN_tags_no = tag.count('NN') / sentences_no
  #Count number of 	adjective in given text / total number of sentences
  JJ_tags_no = tag.count('JJ') / sentences_no
  #Count number of 	adverb in given text / total number of sentences
  RB_tags_no = tag.count('RB') / sentences_no
  #Count number of verb in past tense in given text / total number of sentences
  VBD_tags_no = tag.count('VBD') / sentences_no
  #Count number of verb in gerund or present participle in given text / total number of sentences
  VBG_tags_no = tag.count('VBG') / sentences_no
  #Count number of verb in past participle in given text / total number of sentences
  VBN_tags_no = tag.count('VBN') / sentences_no
  #Count number of verb in non-3rd person singular present in given text / total number of sentences
  VBP_tags_no = tag.count('VBP') / sentences_no

  #Count number of noun as singular or mass or proper singular or proper plural in given text / total number of words
  nouns_no = (pos.count('NOUN') + pos.count('PROPN')) / words_no
  #Count number of noun as proper singular or proper plural in given text / total number of words
  proper_nouns_no = pos.count('PROPN') / words_no
  #Count number of conjunctions and adverbs in given text / total number of words
  conjunctions_no = (pos.count('CONJ') + pos.count('ADP')) / words_no
  #Count number of adjective as comparative and superlative in given text / total number of words
  adjectives_no = pos.count('ADJ') / words_no
  #Count number of non modal verbs in given text / total number of words
  non_modal_verbs_no = (pos.count('VERB') - tag.count('MD')) / words_no 
  #Count number of interjection in given text / total number of words
  interjections_no = pos.count('INTJ') / sentences_no
  #Count number of wh-adverb in given text / total number of words
  adverbs_no = pos.count('ADV') / sentences_no
  #Count number of determiner in given text / total number of words
  determiners_no = pos.count('DET') / sentences_no

  #return a list with text features
  return [ commas_no , pronouns_no , modal_verbs_no , personal_pronouns_no \
  , wh_pronouns_no , function_words_no , VB_tags_no , VBD_tags_no , VBG_tags_no \
  , VBN_tags_no , VBP_tags_no , nouns_no , proper_nouns_no , conjunctions_no \
  , adjectives_no , non_modal_verbs_no , interjections_no , adverbs_no , determiners_no ]

#Test text features with a sentence
text_features("Apple is looking at buying U.K., startup for $1 billion")

#Declare a list of connectives
connectives = ['and', 'also', 'besides', 'further', 'furthermore', 'too', 'moreover', 'in addition', 'then', 'of equal importance', 'equally important', 'another','next', 'afterward', 'finally', 'later', 'last', 'lastly', 'at last', 'now', 'subsequently', 'then', 'when', 'soon', 'thereafter',  'meanwhile', 'following',  'ultimately', 'presently','first', 'second', 'finally', 'hence', 'next', 'then', 'after', 'before', 'gradually','above', 'behind', 'below', 'beyond', 'here', 'there', 'nearby', 'opposite', 'moreover', 'furthermore', 'similarly', 'hence', 'so', 'accordingly', 'consequently', 'thus', 'since', 'therefore', 'conversely', 'however', 'still', 'nevertheless', 'nonetheless', 'or', 'actually','like','similarly','but','contrast', 'conversely', 'however', 'still', 'nevertheless', 'nonetheless', 'yet', 'actually', 'briefly', 'finally']

#Analyze logical words features
def Logical_words(text):
  
  #Use nltk for word tokenizer
  words = nltk.word_tokenize(text)
  
  #Create 4 counters
  ands = 0
  ors = 0
  ifs = 0
  neg = 0
  logic_Operators = 0
  no_connectives = 0

  #Loop on all words
  for word in words:
    #count ands
    if word == 'and':
      ands+=1
    #count ors
    elif word == 'or':
      ors+=1
    #count ifs
    elif word == 'if':
      ifs+=1
    #count not
    elif word == 'not' or word == "n't":
      neg+=1
    #count logical operators
    if word == 'not' or word == "and" or word == "or":
      logic_Operators+=1
    #count connectives
    if word in connectives:
      no_connectives+=1
  
  #return a list with logical words features
  return [ands , ors , ifs , neg , logic_Operators , no_connectives]

#Test logical words features with a sentence
Logical_words('Apple and is looking at or buying U.K., startup for $1 billion')

#Analyse text to return important parameters for all readability fomulas
def get_param(text): 

  #Use spacy lib for tokenization
  nlp = spacy.load('en') 
  doc = nlp(text)
  sentences = doc.sents

  #create 3 counters
  wordsNo = 0
  sentencesNo = 0
  poly_syllable_count = 0

  #Create an empty list for words
  words = []
  for sentence in sentences: 
    #Count all words
    wordsNo += len([token for token in sentence]) 
    #Count all sentences
    sentencesNo += 1
    #create a list of words
    words += [str(token) for token in sentence]

  #Create a difficult words set to contain difficult words
  diff_words = set() 
  #Load easy word set
  easy_word = set([ln.strip() for ln in pkg_resources.resource_stream('textstat', '/resources/en/easy_words.txt')])
  #Loop on all words
  for word in words: 
    #Get syllable count 
    syllable_count = textstatistics().syllable_count(word) 
    #poly_syllable_count is when syllable is grater than three per word
    if syllable_count >= 3:
      poly_syllable_count += 1
    #Difficult word when the word is not in easy word set and contain more than 2 syllables
    if word not in easy_word and syllable_count >= 2: 
      diff_words.add(word) 
   
  #Analyse text to return important parameters for all readability fomulas
  return wordsNo, sentencesNo, len(diff_words), poly_syllable_count

def dale_chall_readability_score(text):
  #Raw score = 0.1579*(Percentage of difficult words) + 0.0496*(Average sentence length) + 3.6365

  words,sentences,difficult_words,_ = get_param(text) 
  #calculate average sentence length
  average_sentence_length = float(words/sentences)
	# Number of words not termed as difficult words 
  not_difficult_words = words - difficult_words 
  if words > 0: 

		# Percentage of words not on difficult word list 
    per_not_difficult_words = float(not_difficult_words) / float(words) * 100
	# diff_words stores percentage of difficult words 
  per_diff_words = 100 - per_not_difficult_words 
  raw_score = (0.1579 * per_diff_words) + (0.0496 * average_sentence_length) 
	
	# If Percentage of Difficult Words is greater than 5 %, then; 
	# Adjusted Score = Raw Score + 3.6365, 
	# otherwise Adjusted Score = Raw Score 

  if per_diff_words > 5:	 
    raw_score += 3.6365
		
  return legacy_round(raw_score, 2) 
  
def flesch_reading_ease(text): 
  #Reading Ease score = 206.835 - (1.015 × average sentence length) - (84.6 × average word length in syllables)

  words_count,sentences_count,_,_ = get_param(text) 
  #calculate average sentence length
  avg_sentence_length = float(words_count/sentences_count)
  syllable_count = textstatistics().syllable_count(text)
  #calculate average syllables per word
  avg_syllables_per_word = float(syllable_count) / float(words_count)

  FRE = 206.835 - float(1.015 * avg_sentence_length) - float(84.6 * avg_syllables_per_word) 

  return legacy_round(FRE, 2) 

def gunning_fog(text): 
  #Grade level= 0.4 * ( (average sentence length) + (percentage of Hard Words) )

  words_count,sentences_count,difficul_words_count,_ = get_param(text)
  #calculate average sentence length
  avg_sentence_length = float(words_count/sentences_count)
  #calculate percentage of difficult words
  per_diff_words = (difficul_words_count / words_count * 100) + 5
  grade = 0.4 * (avg_sentence_length + per_diff_words) 
  return grade

def smog_index(text):
  #SMOG grading = 3 + √(polysyllable count)
  #polysyllable count = number of words of more than two syllables in a sample of 30 sentences

  _,sentence_count,_,poly_syllable_count = get_param(text) 

  if sentence_count >= 3:
    SMOG = (1.043 * (30*(poly_syllable_count / sentence_count))**0.5) \
            + 3.1291
    return legacy_round(SMOG, 2)
  else:
    return 0

#test readability equations
print(dale_chall_readability_score(text_data[500]))
print(flesch_reading_ease(text_data[500]))
print(gunning_fog(text_data[500]))
print(smog_index(text_data[500]))

#calculate all text parameters used in readability equations as features
def text_param(text):

  #Use spacy lib for tokenization 
  nlp = spacy.load('en') 
  doc = nlp(text)
  sentences = doc.sents

  #create 5 counters
  wordsNo = 0
  sentencesNo = 0
  poly_syllable_count = 0
  long_word = 0
  chars = 0

  #Create an empty list for words
  words = []
  for sentence in sentences: 
    #Count all sentences
    sentencesNo += 1
    for token in sentence:
      #Count all words
      wordsNo += 1
      words.append(str(token))
        # print(str(token))

  #Create a difficult words set to contain difficult words
  diff_words = set() 
  #Load easy word set
  easy_word = set([ln.strip() for ln in pkg_resources.resource_stream('textstat', '/resources/en/easy_words.txt')])
  #Load easy word set
  for word in words: 
    #Get syllable count of word
    syllable_count = textstatistics().syllable_count(word) 
    #poly_syllable_count is when syllable is grater than three per word
    if syllable_count >= 3:
      poly_syllable_count += 1
    #Long word is when its length is greater than 7
    if len(word)>7:
      long_word += 1
    #Count no of characters
    chars += len(word)

  #Get syllable count of whole text
  syllable_count = textstatistics().syllable_count(text)
  #Get lexical count of whole text
  lexical_counts = textstat.lexicon_count(text, removepunct=True)
  #calculate average sentence length
  average_sentence_length = float(wordsNo / sentencesNo)
  #calculate average syllables per words
  average_syllables_per_words = float(syllable_count / wordsNo)
  #calculate average poly syllable per words
  average_poly_syllable = float (poly_syllable_count / wordsNo)
  #calculate average long words per words
  average_long_word = float (long_word / wordsNo)
  #calculate average word length
  average_word_length = float (chars / wordsNo)
  
  #return a list with text parameters used in readability equations as features
  return [ wordsNo , sentencesNo, average_sentence_length , syllable_count ,\
          average_syllables_per_words , poly_syllable_count  , lexical_counts ,\
          average_poly_syllable , long_word , average_long_word , average_word_length ]

#convert text_data from a list to data frame
data = pd.DataFrame(text_data)
#create an empty data frame for features
df = pd.DataFrame()

#get logical words features for all rows in data
df[['ands' , 'ors' , 'ifs' , 'neg' , 'logic_Operators' , 'no_connectives']] \
    = data[0].apply(lambda x:pd.Series(Logical_words(x)))

#get text features for all rows in data
df[['commas_number' , 'pronouns_number' , 'modal_verbs_number' , \
    'personal_pronouns_number' , 'wh_pronouns_number' , 'function_words_number' , \
    'VB_tags_number' , 'VBD_tags_number' , 'VBG_tags_number' , 'VBN_tags_number' , \
    'VBP_tags_number' , 'nouns_number' , 'proper_nouns_number' , 'conjunctions_number' , \
    'adjectives_number' , 'non_modal_verbs_number' , 'interjections_number' , \
    'adverbs_number' , 'determiners_number' ]] \
    = data[0].apply(lambda x:pd.Series(text_features(x)))

#calculate all readability equations for all rows in data
df["Flesch_Reading_Ease_score"] = data[0].apply(lambda x:flesch_reading_ease(x))
df["Flesch_Kincaid_Grade_Level"] = data[0].apply(lambda x:textstat.flesch_kincaid_grade(x))
df["Fog_Scale"] = data[0].apply(lambda x:gunning_fog(x))
df["SMOG_Index"] = data[0].apply(lambda x:smog_index(x))
df["Automated_Readability_Index"] = data[0].apply(lambda x:textstat.automated_readability_index(x))
df["Coleman_Liau_Index"] = data[0].apply(lambda x:textstat.coleman_liau_index(x))
df["Linsear_Write_Formula"] = data[0].apply(lambda x:textstat.linsear_write_formula(x))
df["Dale_Chall_Readability_Score"] = data[0].apply(lambda x:dale_chall_readability_score(x))

#get text parameters used in readability equations for all rows in data
df[['Word_count' , 'Sentence_count' , 'Average_Sentence_length' , \
    'Syllable_Count' , 'Average_syllables_per_words' , 'poly_syllable_count' , \
    'Lexical_Count' , 'average_poly_syllable' , 'long_word' , 'average_long_word' , \
    'average_word_length']]  = data[0].apply(lambda x:pd.Series(text_param(x)))

#Drop bad featurs
df = df.drop('long_word',axis=1)
df = df.drop('average_long_word',axis=1)
df = df.drop('ands',axis=1)
df = df.drop('ors',axis=1)
df = df.drop('ifs',axis=1)
df = df.drop('neg',axis=1)
df = df.drop('logic_Operators',axis=1)
df = df.drop('no_connectives',axis=1)

#check for features first 10 elements
df.head(10)

#check for features last 10 elements
df.tail(10)

#convert labels from list to DataFrame
labels = pd.DataFrame(text_labels)

#check for labels and features shape
print(df.shape)
print(labels.shape)

#name features as x and labels as y
X = df
y = labels

#use k fold cross validation for k = 10
kfold = KFold(n_splits=10, shuffle=True, random_state=42)

#Varaibles to determine the best model
max = 0
X_best_model = 0
y_best_model = 0
best_model = 0

#Loop for k times
for train_index, test_index in kfold.split(X):   

  #Split data for training and testing each time randomly
  X_train, X_test = X.loc[train_index], X.loc[test_index]
  y_train, y_test = y.loc[train_index], y.loc[test_index]

  #create xgboost classifier with the following parameters
  modelXGB = xgb.XGBClassifier(objective="multi:softprob",
                               learning_rate =0.3,
                               max_depth=6,
                               min_child_weight=1,
                               gamma=0.0,
                               subsample=1,
                               colsample_bytree=1,
                               scale_pos_weight=1,
                               seed=27, 
                               reg_alpha=0,
                               random_state=42,
                               eval_metric="merror")
  
  #fit the model on training set with early stopping = 5
  modelXGB = modelXGB.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)])

  #test model with test set
  y_pred = modelXGB.predict(X_test)

  #check if this is the best model accuracy
  if(accuracy_score(y_test, y_pred)>max):
    #save the best model with test data
    max = accuracy_score(y_test, y_pred)
    best_model = modelXGB
    X_best_model = X_test
    y_best_model = y_test

  print("Valid accuracy = ",accuracy_score(y_test, y_pred))
  print("________________________________________________")

#Check for accuracy of the best model

y_pred = best_model.predict(X_best_model)

print("model score = ",best_model.score(X_best_model,y_best_model))

print("Valid accuracy = ",accuracy_score(y_best_model, y_pred))

#print the classification report

print(classification_report(y_best_model, y_pred))

# plot feature importance
fig, ax = plt.subplots(figsize=(20, 15))
xgb.plot_importance(best_model,ax=ax)
plt.show()

#plot the decision tree for readability classofocation model

fig, ax = plt.subplots(figsize=(50, 50))
xgb.plot_tree(best_model,rankdir='LR',num_trees=4,ax=ax)
plt.show()

#Save the readability model to use as an api
save_pkl("Readability_model.pkl",best_model)

#Try gradient boosting classifier with k fold when k = 10

kfold = KFold(n_splits=10, shuffle=True, random_state=42)

max = 0
X_best_model = 0
y_best_model = 0
best_model = 0

for train_index, test_index in kfold.split(X):   
  X_train, X_test = X.loc[train_index], X.loc[test_index]
  y_train, y_test = y.loc[train_index], y.loc[test_index]

  modelGB = GradientBoostingClassifier()
  modelGB = modelGB.fit(X_train, y_train)
  
  y_pred = modelGB.predict(X_test)

  if(accuracy_score(y_test, y_pred)>max):
    max = accuracy_score(y_test, y_pred)
    best_model = modelGB
    X_best_model = X_test
    y_best_model = y_test

  print("Valid accuracy = ",accuracy_score(y_test, y_pred))
  print("________________________________________________")

y_pred = best_model.predict(X_best_model)

print("model score = ",best_model.score(X_best_model,y_best_model))

print("Valid accuracy = ",accuracy_score(y_best_model, y_pred))

#Try lightgbm classifier with k fold when k = 10
kfold = KFold(n_splits=10, shuffle=True, random_state=42)

max = 0
X_best_model = 0
y_best_model = 0
best_model = 0

for train_index, test_index in kfold.split(X):   
  X_train, X_test = X.loc[train_index], X.loc[test_index]
  y_train, y_test = y.loc[train_index], y.loc[test_index]

  modelLGBM = LGBMClassifier()
  modelLGBM = modelLGBM.fit(X_train, y_train)
  
  y_pred = modelLGBM.predict(X_test)

  if(accuracy_score(y_test, y_pred)>max):
    max = accuracy_score(y_test, y_pred)
    best_model = modelLGBM
    X_best_model = X_test
    y_best_model = y_test

  print("Valid accuracy = ",accuracy_score(y_test, y_pred))
  print("________________________________________________")

y_pred = best_model.predict(X_best_model)

print("model score = ",best_model.score(X_best_model,y_best_model))

print("Valid accuracy = ",accuracy_score(y_best_model, y_pred))

#Try KNeighborsClassifier with k fold when k = 10

kfold = KFold(n_splits=10, shuffle=True, random_state=42)

max = 0
X_best_model = 0
y_best_model = 0
best_model = 0

for train_index, test_index in kfold.split(X):   
  X_train, X_test = X.loc[train_index], X.loc[test_index]
  y_train, y_test = y.loc[train_index], y.loc[test_index]

  modelKNC = KNeighborsClassifier(n_neighbors=5)
  modelKNC = modelKNC.fit(X_train, y_train)
  
  y_pred = modelKNC.predict(X_test)

  if(accuracy_score(y_test, y_pred)>max):
    max = accuracy_score(y_test, y_pred)
    best_model = modelKNC
    X_best_model = X_test
    y_best_model = y_test

  print("Valid accuracy = ",accuracy_score(y_test, y_pred))
  print("________________________________________________")

y_pred = best_model.predict(X_best_model)

print("model score = ",best_model.score(X_best_model,y_best_model))

print("Valid accuracy = ",accuracy_score(y_best_model, y_pred))

#Try DecisionTreeClassifier with k fold when k = 10

kfold = KFold(n_splits=10, shuffle=True, random_state=42)

max = 0
X_best_model = 0
y_best_model = 0
best_model = 0

for train_index, test_index in kfold.split(X):   
  X_train, X_test = X.loc[train_index], X.loc[test_index]
  y_train, y_test = y.loc[train_index], y.loc[test_index]

  clf = DecisionTreeClassifier(random_state=0)
  clf = clf.fit(X_train, y_train)
  
  y_pred = clf.predict(X_test)

  if(accuracy_score(y_test, y_pred)>max):
    max = accuracy_score(y_test, y_pred)
    best_model = clf
    X_best_model = X_test
    y_best_model = y_test

  print("Valid accuracy = ",accuracy_score(y_test, y_pred))
  print("________________________________________________")

y_pred = best_model.predict(X_best_model)

print("model score = ",best_model.score(X_best_model,y_best_model))

print("Valid accuracy = ",accuracy_score(y_best_model, y_pred))

