{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBXDVamzQjJj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut,KFold, StratifiedKFold\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "#from google.colab import drive\n",
    "from keras import backend as K\n",
    "from random import shuffle\n",
    "import itertools as tools\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "#drive.mount('/content/gdrive', force_remount=True)\n",
    "dataset_path = './public/processed data/csv data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRH0O-WNrFmP"
   },
   "source": [
    "### Helper functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MsBPwQFWqU4p"
   },
   "source": [
    "Pre-processor: includes all functions that handle tokenization and word embeddings part of processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2pvbKl3SHLo"
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, embidding_dims= 100, max_sequence_length = 100, padding_type='post'):\n",
    "        \n",
    "        self.tokenizer = None\n",
    "        self.embeddings_matrix = None\n",
    "        self.embedding_dim = embidding_dims\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.padding_type = padding_type\n",
    "        \n",
    "    def __make_tokenizer(self, text, oov_token='<OOV>'):\n",
    "        '''\n",
    "            make tokenizer from sentences\n",
    "            \n",
    "            -- inputs:\n",
    "                sentences:text to fit tokenizer on\n",
    "                oov_token: out of vocabulary token \n",
    "            -- returns:\n",
    "                None\n",
    "        '''\n",
    "        self.tokenizer = Tokenizer(oov_token=oov_token)\n",
    "        self.tokenizer.fit_on_texts(text)    \n",
    "    \n",
    "    def load_tokenizer(self, file_path):\n",
    "        '''\n",
    "            load tokenizer from pickle file\n",
    "            \n",
    "            -- inputs:\n",
    "                file_path: path to the file of tokenizer \n",
    "            -- returns:\n",
    "                None\n",
    "        '''\n",
    "        try:\n",
    "            self.tokenizer = pkl.load(open(file_path,'rb'))\n",
    "        except  Exception  as e :\n",
    "            print(f'could not load tokenizer from path: {file_path}\\n{e}')\n",
    "    \n",
    "    \n",
    "    def make_tokenizer(self, dataset_path, save_tokenizer=False, tokenizer_file_path=None):\n",
    "        '''\n",
    "            make a tokenizer from separate files\n",
    "            \n",
    "            -- inputs:\n",
    "                file_path: path to the file of tokenizer \n",
    "            -- returns:\n",
    "                None\n",
    "        '''\n",
    "        df = pd.read_csv(f'{dataset_path}', sep=',')\n",
    "        \n",
    "        self.tokenizer = self.__make_tokenizer(np.array(dataframe['data']))\n",
    "\n",
    "        if save_tokenizer == True:\n",
    "            try:\n",
    "                pkl.dump(tokenizer, open(f'{tokenizer_file_path}', 'wb'))\n",
    "            except Exception  as e :\n",
    "                print(f'could not save tokenizer to path: {tokenizer_file_path}\\n{e}')\n",
    "    \n",
    "    def make_embeddings(self, path_to_embeddings= dataset_path + 'glove.6B.100d.txt'):\n",
    "        \n",
    "        if self.tokenizer == None:\n",
    "            print('could not create embeddings matrix from empty tokenizer')\n",
    "        \n",
    "        else:  \n",
    "            embeddings_index = {}\n",
    "            vocab_size=len(self.tokenizer.word_index)\n",
    "\n",
    "            with open(f'{path_to_embeddings}', encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    values = line.split()\n",
    "                    word = values[0]\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings_index[word] = coefs\n",
    "\n",
    "            embedding_index = np.zeros((vocab_size+1, self.embedding_dim))\n",
    "            for word, i in self.tokenizer.word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_index[i] = embedding_vector\n",
    "\n",
    "            self.embeddings_matrix = embedding_index\n",
    "        \n",
    "    def tokenize_data(self, paragraphs):          \n",
    "        sequence = self.tokenizer.texts_to_sequences(paragraphs)\n",
    "        padded_sequence = pad_sequences(sequence, padding=self.padding_type,maxlen=self.max_sequence_length)\n",
    "        return padded_sequence        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDHhM-iZq6BI"
   },
   "source": [
    "### Data transformer: includes all functions that handle datsaet manipulations\n",
    "\n",
    "\n",
    "*   generate n-permutations of file\n",
    "*   clean up non-ascii characters from text line\n",
    "*   read file from directory\n",
    "*   generate paragraph-label pairs from data directory\n",
    "*   remove non-ascii characters from text line\n",
    "*   generate paragraphs of n-sentences from file\n",
    "*   generate paragraphs of n-sentences from file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cXAtxDIRYp1"
   },
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __permute_file(self, file_lines, size=10):\n",
    "        '''\n",
    "            returns all permutations of a file\n",
    "\n",
    "            -- inputs:\n",
    "                file_lines: list of file lines\n",
    "                size: number of permutations to return, other than the original one\n",
    "            -- returns:\n",
    "                all permutations of the list element where the original ordering is the first element\n",
    "        '''\n",
    "        file_lines = np.array(file_lines)\n",
    "        indeces = [i for i in range(file_lines.shape[0])]\n",
    "        shuffled_set = set() \n",
    "        while len(shuffled_set) <size:\n",
    "            shuffle(indeces)\n",
    "            shuffled_set.add(tuple(indeces))\n",
    "\n",
    "        permuted_lines = [file_lines]\n",
    "        for idx in shuffled_set:\n",
    "            permuted_lines.append(file_lines[list(idx)])\n",
    "\n",
    "        return permuted_lines\n",
    "\n",
    "    def __remove_non_ascii(self, text):\n",
    "        return ''.join([i if ord(i) < 128 and i not in ['.', '\\n', ','] else '' for i in text])\n",
    "\n",
    "    def __read_file(self, file_path, skip_first_token=False):\n",
    "        '''\n",
    "            reads the lines from file\n",
    "\n",
    "            -- inputs: \n",
    "                file_path: full path to the file to be permuted\n",
    "                skip_first_token: boolean used to sanitize the inputs from the DUC dataset\n",
    "            -- returns:\n",
    "                list of the file lines\n",
    "        '''\n",
    "        #try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            if skip_first_token == True:\n",
    "                return [ self.__remove_non_ascii(line[line.find(' ') + 1:]) for line in file.readlines() ]\n",
    "            return [ self.__remove_non_ascii(line) for line in file.readlines()]\n",
    "        #except:\n",
    "            '''file is either not found or path is wrong\n",
    "            print(f\"file {file_path} was not found!\")\n",
    "            return []'''\n",
    "\n",
    "    def __get_file_labeled_permutations(self, file_path):\n",
    "        '''\n",
    "            generate all permutations of the file, label the original one as coherent, all other permutations are non-coherent\n",
    "\n",
    "            -- inputs: \n",
    "                file_path: full path to the file to be permuted\n",
    "            -- returns: \n",
    "                list of tuples of structure: (lines permutation, label: being 1 for coherent, 0 for non-coherent)\n",
    "        '''\n",
    "        file_lines = self.__read_file(file_path, skip_first_token=True)\n",
    "        permuted_lines = self.__permute_file(file_lines, size=20)\n",
    "        labels = [1] + [0 for i in range(len(permuted_lines)-1)]\n",
    "        return zip(permuted_lines, labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_file_cliques(file_lines, size=3):\n",
    "\n",
    "        '''\n",
    "        divides the file into cliques of similar size\n",
    "\n",
    "        -- inputs: \n",
    "            file_lines: list of file lines to be permuted\n",
    "            size: size of cliques \n",
    "        -- returns:\n",
    "            list of cliques generated\n",
    "        '''\n",
    "        cliques = []\n",
    "        for idx in range(len(file_lines)-size):\n",
    "            current_clique = []\n",
    "            for increment in range(size):\n",
    "                current_clique.append(file_lines[idx+increment])\n",
    "            cliques.append(current_clique)\n",
    "\n",
    "        return cliques\n",
    "    \n",
    "    def generate_separate_files_method(self, dataset_path, clique_size = 3):\n",
    "        '''\n",
    "            file structure is: n+1 lines where the first line is either 1 for coherent documents, 0 for non coherent\n",
    "                followed by n-lines of the document.\n",
    "        '''\n",
    "        count = 0\n",
    "        print(f'reading files from {dataset_path}')\n",
    "        for (_, _, file_names) in walk(dataset_path):\n",
    "            for file_name in file_names:\n",
    "                for file_lines, label in self.__get_file_labeled_permutations(dataset_path+file_name):\n",
    "                    for clique in DataTransformer.generate_file_cliques(file_lines, size=clique_size):\n",
    "                        with open(f'./processed data/separate-files/{count}.txt','w') as file:\n",
    "                            file.write(f'{label}. ')\n",
    "                            for line in clique:\n",
    "                                file.write(f'{line}. ')\n",
    "                        count += 1\n",
    "                        \n",
    "    def generate_csv_dataset_from_separate_files (self, dataset_path, file_name):\n",
    "        \n",
    "        with open(f'{dataset_path}{file_name}', 'a') as csv_file:\n",
    "            csv_file.write('data,label\\n')\n",
    "\n",
    "            for (_, _, file_names) in walk(dataset_path):\n",
    "                for file_name in file_names:\n",
    "                    file_lines = open(f'{dataset_path}{file_name}', 'r').readline().split('.')[:-1]\n",
    "                    if len(file_lines) != 4:\n",
    "                        print(file_name)\n",
    "                    else:\n",
    "                        for line in file_lines[1:]:\n",
    "                            csv_file.write(f'{line}. ')\n",
    "                        csv_file.write(f',{file_lines[0]}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzZh1dRdtCfe"
   },
   "source": [
    "### Model: includes all model related functionalities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFLb-J1LtQ7q"
   },
   "source": [
    "Similarity matrix:\n",
    "trainable matrix M that captures similarity between two sentences according to the equation: \n",
    "\n",
    " ![similarity function](https://drive.google.com/uc?id=1y_ojFiDHkrbOwi7LEXrGo2UTXo4Qbv5o)\n",
    "\n",
    "where: \n",
    "*   Xf: first sentence\n",
    "*   Xs: second sentence\n",
    "*   M: similarity matrix (trainable weights)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lApxKxBOU5OF"
   },
   "outputs": [],
   "source": [
    "class SimilarityMatrix(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,dims, **kwargs):\n",
    "        self.dims_length, self.dims_width = dims\n",
    "        super(SimilarityMatrix, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self._m = self.add_weight(name='M', \n",
    "                                    shape=(self.dims_length,self.dims_width),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "        super(SimilarityMatrix, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, y): \n",
    "        xf, xs = y\n",
    "        sim1=tf.matmul(xf, self._m)\n",
    "        transposed = tf.reshape(K.transpose(xs),(-1, 100, 1))\n",
    "        sim2=tf.matmul(sim1, transposed)\n",
    "        return sim2\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (1)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'dims_length': self.dims_length, \n",
    "            'dims_width': self.dims_width\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IB9tzXpzv03g"
   },
   "source": [
    "Model Helper: includes all training related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFdnW67fpYPJ"
   },
   "outputs": [],
   "source": [
    "class ModelHelper:\n",
    "  \n",
    "    @staticmethod\n",
    "    def negative_log_likelihood(y_true, y_pred):\n",
    "        \n",
    "        '''\n",
    "          Calculates negative log likelihood\n",
    "\n",
    "          -- inputs: \n",
    "              y_true: ground truth  values\n",
    "              y_predictions: non categorical predicted values\n",
    "              y_pred: (optional) title for the plot\n",
    "          -- returns:\n",
    "              negative likelihood total loss \n",
    "        '''\n",
    "        return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_ROC(y_true, y_predictions, title=''):\n",
    "\n",
    "        '''\n",
    "          Plot ROC curve\n",
    "          \n",
    "          -- inputs: \n",
    "              y_true: ground truth  values\n",
    "              y_predictions: non categorical predicted values\n",
    "              title: (optional) title for the plot\n",
    "          -- returns:\n",
    "              None\n",
    "        '''\n",
    "\n",
    "        ## calculate the FPR, TPR, Thresholds and AUC value\n",
    "        false_pos_rate, true_pos_rate, thresholds = roc_curve(y_true, y_predictions)\n",
    "        auc_val = auc(false_pos_rate, true_pos_rate)\n",
    "\n",
    "        ## plot ROC curve\n",
    "        plt.figure(1)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.plot(false_pos_rate, true_pos_rate, label=f'{title}' +' (area = {:.3f})'.format(auc_val))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.title('ROC curve')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_precision_recall(model, X_data, y_true):\n",
    "      \n",
    "        '''\n",
    "          Plot precision-recall curve\n",
    "\n",
    "          -- inputs: \n",
    "              model: model for which curve is plotted\n",
    "              X_data: features to predict \n",
    "              y_true: ground truth  values\n",
    "          -- returns:\n",
    "              None\n",
    "        '''\n",
    "        pred = model.predict(X_data).ravel()\n",
    "\n",
    "        average_precision = average_precision_score(y_true, pred)\n",
    "\n",
    "        disp = plot_precision_recall_curve(model, X_data, y_true)\n",
    "        disp.ax_.set_title('binary Precision-Recall curve: ' + 'AP={0:0.2f}'.format(average_precision))\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_model(model , loss_func, monitor_metrics = ['acc'], optimizer='adam'):\n",
    "      \n",
    "        '''\n",
    "            Compile model\n",
    "            \n",
    "            -- inputs: \n",
    "                model: model to compile\n",
    "                loss_func: loss function to be used\n",
    "                monitor_metrics: (optional) metrics to be monitored \n",
    "                optimizer: (optional) optimizer to use, adam is the default\n",
    "            -- returns:\n",
    "                None\n",
    "        '''\n",
    "        model.compile(optimizer=optimizer, loss=loss_func, metrics=monitor_metrics)   \n",
    "\n",
    "    @staticmethod\n",
    "    def train_model_kfolds(data, model_class, loss_func, num_of_folds, verbose=2, batch_size=128, plot_roc = False, plot_prec_recall = False ):\n",
    "        \n",
    "        model_callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "            #tf.keras.callbacks.TensorBoard(log_dir=r'.\\ogs') \n",
    "        ]\n",
    "        \n",
    "        X_data, y_data = data[0].astype(np.float32), data[1].astype(np.float32)\n",
    "        \n",
    "        count = 0\n",
    "\n",
    "        for train_index, test_index in StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=42).split(X_data, y_data):\n",
    "            \n",
    "            X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "            \n",
    "            y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "            \n",
    "            model = model_class()\n",
    "            model.make_model()\n",
    "            model = model.model\n",
    "\n",
    "            ModelHelper.compile_model(model, ModelHelper.negative_log_likelihood)\n",
    "\n",
    "            model.fit(X_train,y_train,validation_data=(X_test,y_test),verbose=verbose,epochs=20, batch_size=batch_size, callbacks=model_callbacks)\n",
    "            \n",
    "\n",
    "            pred = model.predict(X_test).ravel()\n",
    "            \n",
    "            loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "            print(f'fold #{count+1} test loss: {loss}, test acc: {acc}')\n",
    "\n",
    "            average_precision = average_precision_score(y_test, pred)\n",
    "\n",
    "            print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "            \n",
    "            if plot_roc:\n",
    "              ModelHelper.plot_ROC(y_test, pred, 'test data')\n",
    "            if plot_prec_recall:\n",
    "              ModelHelper.plot_precision_recall(model, X_test, y_test)  \n",
    "\n",
    "            count += 1\n",
    "            gc.collect()\n",
    "\n",
    "    @staticmethod\n",
    "    def train_model(data, model, loss_func, epoches=100, verbose=1, batch_size=128, early_stop = False):\n",
    "        \n",
    "        model_callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath=dataset_path+'models/model.{epoch:03d}-{val_loss:.4f}.h5'),\n",
    "            #tf.keras.callbacks.TensorBoard(log_dir=r'.\\ogs') \n",
    "        ]\n",
    "\n",
    "        if early_stop:\n",
    "          model_callbacks.append( \n",
    "            tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "          )\n",
    "        \n",
    "        X_data, y_data = data[0].astype(np.float32), data[1].astype(np.float32)\n",
    "        \n",
    "\n",
    "        ModelHelper.compile_model(model, loss_func)\n",
    "\n",
    "        model.fit(X_data,y_data,verbose=verbose,epochs=epoches, batch_size=batch_size, callbacks=model_callbacks, validation_split=0.15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPij43Uev_k6"
   },
   "source": [
    "Model: includes all the CNN related functions\n",
    "the make_model function builds a Convolutional Neural Net according to the architecture suggested by the paper as shown in the figure below\n",
    "\n",
    "![cnn model architecture](https://drive.google.com/uc?id=118Olwuh9VL5_Rt_IBg6G5JfuT2KEl-Z5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZu6Ahs_SFto"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "      \n",
    "        '''\n",
    "          draw the ROC curve\n",
    "            \n",
    "          -- inputs: \n",
    "              None\n",
    "          -- returns:\n",
    "              None\n",
    "        '''\n",
    "        self.num_of_folds = int(5)\n",
    "        self.dataset = None\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        self.test_data = None\n",
    "        \n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.preprocessor.load_tokenizer(file_path = dataset_path + 'tokenizer1.pkl')\n",
    "        self.preprocessor.make_embeddings()\n",
    "    \n",
    "    def make_dataset(self):\n",
    "\n",
    "        '''\n",
    "          Make tensorflow eager dataset object from the loaded data to model\n",
    "\n",
    "          -- inputs: \n",
    "              None\n",
    "          -- returns:\n",
    "              None\n",
    "        '''\n",
    "\n",
    "        if self.data == None:\n",
    "            print('cannot create dataset from empty data object, please load data first then create the dataset iterator')\n",
    "        \n",
    "        else:\n",
    "            X_data, y_data = self.data[0], self.data[1]\n",
    "            \n",
    "            def generator():\n",
    "                for train_index, test_index in KFold(n_splits=self.num_of_folds).split(X_data):\n",
    "                    X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "                    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "                    yield X_train,y_train,X_test,y_test\n",
    "\n",
    "            self.dataset =  tf.data.Dataset.from_generator(generator, (tf.string,tf.int64,tf.string,tf.int64))\n",
    "    \n",
    "    \n",
    "\n",
    "    def make_model(self):\n",
    "\n",
    "        '''\n",
    "          Make keras CNN model\n",
    "\n",
    "          -- inputs: \n",
    "              None\n",
    "          -- returns:\n",
    "              None\n",
    "        '''\n",
    "\n",
    "        X_input =  tf.keras.Input(shape=(3, 100), name=\"input-sentences\")\n",
    "        \n",
    "        \n",
    "        embedding_layer = tf.keras.layers.Embedding(input_dim= len(self.preprocessor.tokenizer.word_index)+1, \n",
    "                                                    output_dim=self.preprocessor.embedding_dim, \n",
    "                                                    input_length=self.preprocessor.max_sequence_length,\n",
    "                                                    trainable = False,\n",
    "                                                    name='glove-embedding-layer')\n",
    "        embedding_layer.build((None,))\n",
    "        embedding_layer.set_weights([self.preprocessor.embeddings_matrix])\n",
    "        \n",
    "        first_sentence =  embedding_layer(X_input[:,0,:])\n",
    "        second_sentence =  embedding_layer(X_input[:,1,:])\n",
    "        third_sentence =  embedding_layer(X_input[:,2,:])\n",
    "        \n",
    "        convolutional_filters_map = tf.keras.layers.Conv1D(100,kernel_size=(3), activation='relu', use_bias=True, name='features-map')\n",
    "        \n",
    "        Xf = convolutional_filters_map(first_sentence)\n",
    "        Xs = convolutional_filters_map(second_sentence)         \n",
    "        Xt = convolutional_filters_map(third_sentence)   \n",
    "\n",
    "        Xf = tf.keras.layers.MaxPool1D(98, name='first-sentence-pool')(Xf)\n",
    "        Xs = tf.keras.layers.MaxPool1D(98, name='second-sentence-pool')(Xs)\n",
    "        Xt = tf.keras.layers.MaxPool1D(98, name='third-sentence-pool')(Xt)\n",
    "\n",
    "        similarity_fnc = SimilarityMatrix((100,100))\n",
    "\n",
    "        sim_fs = similarity_fnc([Xf, Xs])\n",
    "        sim_st = similarity_fnc([Xs, Xt])\n",
    "\n",
    "        X = tf.keras.layers.concatenate([Xf, sim_fs, Xs, sim_st, Xt])\n",
    "\n",
    "        ## TODO: this architecture requires grad search hyper-parameters tuning\n",
    "        X = tf.keras.layers.Dense(256, activation='relu', name='fc1', use_bias=True)(X)\n",
    "        X = tf.keras.layers.Dropout(0.333)(X)\n",
    "\n",
    "        X = tf.keras.layers.Dense(512, activation='relu', name='fc2', use_bias=True)(X)\n",
    "        X = tf.keras.layers.Dropout(0.333)(X)\n",
    "\n",
    "        X = tf.keras.layers.Dense(512, activation='relu', name='fc3', use_bias=True)(X)\n",
    "        X = tf.keras.layers.Dropout(0.333)(X)\n",
    "\n",
    "        X = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(X)\n",
    "\n",
    "        model = tf.keras.Model(inputs=[X_input], outputs=[X])\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "    def load_data_from_csv(self, data_path, separator=',', split_train_test=False, make_balanced=False):\n",
    "        \n",
    "        '''\n",
    "            load data from CSV file into dataframe\n",
    "            \n",
    "            -- inputs:\n",
    "                data_path: path to file where data is saved\n",
    "                separator (optional): value seprator to the file, default is comma\n",
    "        '''\n",
    "        \n",
    "        self.data = pd.read_csv(f'{data_path}', sep=',')\n",
    "        self.data['data'] = self.data['data'].apply(lambda x: x.strip().split('.')[:3])\n",
    "        self.data['data'] = self.data['data'].apply(lambda x: self.preprocessor.tokenize_data(x))\n",
    "\n",
    "        if make_balanced:\n",
    "          freq = list(df['label'].value_counts())\n",
    "          freq = freq[0]//freq[1]-1\n",
    "          \n",
    "          df_coherent = self.data.loc[self.data['label'] == 1]\n",
    "          df_coherent_replecated = pd.concat([df_coherent]*freq, ignore_index=True)\n",
    "          self.data = pd.concat([df_coherent_replecated, self.data], ignore_index=True)\n",
    "        \n",
    "        if split_train_test:\n",
    "\n",
    "          X_train, X_test, y_train, y_test = train_test_split(np.array(self.data['data'].values.tolist()), np.array(self.data['label'].values.tolist()).reshape(-1,1), test_size=0.2, random_state=42)\n",
    "          self.data = (X_train, y_train)\n",
    "          self.test_data = ( X_test, y_test)  \n",
    "        else:\n",
    "          self.data = (np.array(self.data['data'].values.tolist()), np.array(self.data['label'].values.tolist()).reshape(-1,1))\n",
    "\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5mszFz09p_J6"
   },
   "source": [
    "###training over the imbalanced dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ckul1uZJTIH3"
   },
   "outputs": [],
   "source": [
    "m = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCbZ3zWfvIyA"
   },
   "outputs": [],
   "source": [
    "m.load_data_from_csv(dataset_path+'dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "uDkmf19kmodJ",
    "outputId": "6ecd5c4b-f7be-457b-e0a2-d9c3573122ea"
   },
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(m.data[1], return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lTJ90gbVmmXm",
    "outputId": "dfab2e2e-c0dc-4d52-c650-e69f1f0e952c"
   },
   "outputs": [],
   "source": [
    "ModelHelper.train_model_kfolds(m.data, Model, ModelHelper.negative_log_likelihood, m.num_of_folds, plot_roc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "oAZxEk-1xU6e",
    "outputId": "81bc432d-22d0-442a-cf2e-371742f0cb6a"
   },
   "outputs": [],
   "source": [
    "m.load_data_from_csv(dataset_path+'dataset.csv', make_balanced=True)\n",
    "unique_elements, counts_elements = np.unique(m.data[1], return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "colab_type": "code",
    "id": "jnKPw5jYxcnU",
    "outputId": "f8a3d7cc-41b0-4ce2-b452-1795a66f5a12"
   },
   "outputs": [],
   "source": [
    "ModelHelper.train_model_kfolds(m.data, Model, ModelHelper.negative_log_likelihood, m.num_of_folds, plot_roc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0pTdVWdxeIU"
   },
   "source": [
    "###Train actual model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1gbF4MLfngo"
   },
   "outputs": [],
   "source": [
    "m.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "XZ34LM58fggW",
    "outputId": "95b3e6b8-38bc-48e7-8140-935cdf18a865"
   },
   "outputs": [],
   "source": [
    "ModelHelper.train_model(m.data, m.model, ModelHelper.negative_log_likelihood, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "EuF535EFjkAY",
    "outputId": "159d9c81-ce69-4e15-cbe4-b3a965dac351"
   },
   "outputs": [],
   "source": [
    "saveloc = dataset_path + \"tmp/model_3_sentence_clique_1.h5\"\n",
    "m.model.save(saveloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnlDGFhcvL3q"
   },
   "outputs": [],
   "source": [
    "ModelHelper.train_model_kfolds(m.data, Model, ModelHelper.negative_log_likelihood, m.num_of_folds, plot_roc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3jBhFm4jgR2"
   },
   "outputs": [],
   "source": [
    "m.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dy4EXKyBaJCf"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(m.model, 'multi_input_and_output_model.png', show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}